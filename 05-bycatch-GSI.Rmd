---
title: "05-bycatch-GSI-assignments"
output: html_notebook
---

This is for the genetic stock identification using rubias with the Northern Fulmar dataset: reference samples were collected at four breeding colonies and mixture samples are bycatch from the Alaskan longline fisheries NOAA Observer Program.

## Background

The background for this analysis is that I have large differences in sample sizes for my fulmar baseline colony dataset and that I know this can cause biased results in assignments - the colonies with smaller sample sizes often lose out.

After a few tests, I determined that downsampling the large colonies changes the distribution of bycatch assignments, with an increased share of the bycatch assigned to the small colonies. This seems to validate the idea that unequal baseline sample sizes are biasing bycatch assignments.

Another set of tests identified 58 (the number of samples for the second-smallest baseline colony) as a much better fit for downsampling than the smallest baseline (36 samples). Downsampling all four colonies to 36 samples eliminated too much of the genotype information.

When downsampling, the problem was that depending on what set.seed was used, different bycatch birds were assigned to colony-of-origin or left unassigned at the 90% probability of assignment threshold.


My plan here is to iterate over multiple set.seed values to generate a dataset that combines the subsampled datasets generated by each downsampled set.seed analysis.


Input bycatch genotypes were cleaned up in `04-tidy-bycatch-genos.Rmd` prior to this analysis.



```{r load-data-and-libraries}
source("R/nofu-functions.R")

library(tidyverse)
library(CKMRsim)
library(stringr)

# I'm reading in the filtered genotype data from `04-tidy-bycatch-genos.Rmd`
no_hi_missers <- read_csv("csv_outputs/bycatch_no_hi_missers_hwe.csv") %>%
  ungroup()
meta <- readRDS("bycatch_data/meta-data-tibble.rds")

```


Before I can perform the rubias assignment...

1. designate the NMFS_DNA_IDs that belong to the mixture and baseline data frames
2. combine genotypes from both data sets to turn alleles into integers and then spread them into the right format to run rubias.

Mixture samples:
```{r add-mixture-column}
# create a data frame for the mixture samples with the sample_type, repunit, and collection columns
mix_ids <- no_hi_missers %>% 
  dplyr::select(NMFS_DNA_ID) %>%
  unique() %>%
  mutate(sample_type = "mixture") %>%
  mutate(repunit = NA)  %>%
  mutate(collection = "bycatch")

# reorder columns
mixture_ids <- mix_ids[,c(2:4,1)]

```

Baseline samples:
```{r read-in-baseline-genos}
# filtered genotypes
baseline <- read_csv("csv_outputs/baseline_no_hi_missers_clean_hwe.csv") %>%
  ungroup()

# sample ids to include for rubias
ref_samples <- read_csv("bycatch_data/baseline_ref_samples_04092019.csv")

# reorder columns
baseline_ids <- ref_samples[, c(2,3,4,1)]

```

517 samples in the ref_samples df
517 in the baseline_no_hi_missers_clean_hwe df as well


## Downsample
Here is where I will take a random slice of the baseline_ids from the colonies with many more samples - the Pribs and the Semidis.

```{r group-numbers}
baseline_ids %>%
  group_by(repunit) %>%
  tally() 

```

Chagulak has the fewest number of samples, with 36, but that removes so much information when I tested so let's equalize to St. Matthew, with 58 samples.

This means I need to split out the Chagulak samples prior to downsampling because there are < 58 samples.


## iterative GSI for the bycatch samples

I wrapped the iterative downsampling into a function. I'll use that with 100 reps, keeping the indivs and colony assignments for 90% probability of assignment.

```{r}
nreps <- 100 # 100 iterations
set.seed(321)
combined_ds_result <- lapply(1:nreps, function(x) downsample_baseline(base_ids = baseline_ids, n_dsample = 58, base_genos = baseline, no_hi_missers = no_hi_missers)[[1]]) %>%
   bind_rows(.id = "iter")

```

summarize those results a bit
```{r}
combined_ds_result %>%
  arrange(indiv) %>%
  group_by(indiv) %>% # if these are unique, there should never be multiple entries for the same sample with different collections
    add_tally(name = "total") %>% # the maximum total number is 100 (the number of reps)
  ungroup() %>%
  group_by(indiv, collection) %>%
  add_tally(name = "n_colony") %>% # this is the number of reps in which this indiv was assigned to this colony 
  mutate(prop_colony = n_colony/total) %>% # the proportion of assignments of this indiv to this colony
  ungroup() %>%
  select(indiv, collection, prop_colony) %>%
  unique() %>%
  ggplot(aes(x = prop_colony)) +
  geom_density()

```


Now take that result and determine which indivs have assignment to a single colony 90% of the time
```{r}
ds_bycatch_colonies <- combined_ds_result %>%
  arrange(indiv) %>%
  group_by(indiv) %>% # if these are unique, there should never be multiple entries for the same sample with different collections
    add_tally(name = "total") %>% # the maximum total number is 100 (the number of reps)
  ungroup() %>%
  group_by(indiv, collection) %>%
  add_tally(name = "n_colony") %>% # this is the number of reps in which this indiv was assigned to this colony 
  mutate(prop_colony = n_colony/total) %>% # the proportion of assignments of this indiv to this colony
  ungroup() %>%
  filter(prop_colony > 0.1) %>% # keep only indivs that were assigned to this colony 90% of the time
  select(indiv, collection) %>%
  unique() 

# just the colony assignments that are > 90% 
keepers <- ds_bycatch_colonies %>%
  group_by(indiv) %>%
  tally() %>%
  filter(n == 1) %>%
  select(-n) %>%
  left_join(ds_bycatch_colonies)

# taking just those birds with a top colony assignment > 90% and adding back on the info about the PofZ
downsampled_bycatch_dataset <- keepers %>%
  left_join(., combined_ds_result) %>% 
  select(indiv, collection, PofZ, z_score) %>% # take the top PofZ per indiv/colony
  group_by(indiv) %>%
  mutate(rank = rank(-PofZ, ties.method = "min")) %>%
  mutate(top_rank = min(rank)) %>%
  filter(rank == top_rank) %>%
  #tally() # there is only a single entry per indiv-collection
  select(-rank, -top_rank) %>%
  left_join(., meta, by = c("indiv" = "NMFS_DNA_ID")) %>%
  select(SAMPLE_ID, indiv, collection, PofZ, z_score) %>%
  rename(NMFS_DNA_ID = indiv) %>%
  ungroup()
  
```


```{r}
# save that output
downsampled_bycatch_dataset %>%
  write_csv("csv_outputs/nofu_bycatch_downsampled_06122020.csv")

```

```{r}
# calculate proportion of the retained bycatch that comes from each colony
downsampled_bycatch_dataset %>%
  group_by(collection) %>%
  tally() %>%
  ungroup() %>%
  mutate(prop = n/sum(n))

```


## Temporal stability


Take the samples that were assigned to colonies at >90%. 

```{r}
downsampled_bycatch_dataset

baseline_ids

baseline
```

The cumulative NMFS ids that we want to use in this self-assignment:
```{r}
# make one big df for self-assignment
combined_df <- bind_rows(baseline, no_hi_missers)

# allele frequencies
kg2 <- combined_df %>% 
  select(NMFS_DNA_ID, locus, allele) %>%
  mutate(Chrom = "GTseq") %>% 
  mutate(Pos = as.integer(factor(locus, levels = unique(locus)))) %>%
  rename(Locus = locus,
         Allele = allele) %>%
  select(NMFS_DNA_ID, Chrom, Locus, Pos, Allele) %>%
  ungroup()

# get the allele freqs
kg_ckmr_markers <- kg2 %>%
  filter(!is.na(Allele)) %>% # it is vital to filter out the NAs at this stage
  group_by(Chrom, Locus, Pos, Allele) %>%
  summarise(counts = n()) %>%
  group_by(Locus, Pos) %>%
  mutate(Freq = counts / sum(counts)) %>%
  select(-counts) %>%
  mutate(AlleIdx = 1,
         LocIdx = 1) %>%
  reindex_markers(.)

# summary stats
kg_ckmr_markers %>%
  group_by(Locus) %>%
  tally() %>%
  arrange(n) %>%
  summarise(mean(n))

```

Remove the 8 loci that are out of HWE in three or more of the four populations. 
```{r}
# loci that deviate
outliers <- read_csv("csv_outputs/loci_out_hwe.csv")

outliers$Locus <- gsub("_1", "", outliers$Locus)
```

```{r}
# replace the weird characters in the locus name to match
combined_df$locus <- gsub(":", "_", combined_df$locus)
combined_df$locus <- gsub("-", "_", combined_df$locus)
```

```{r}
# now anti-join the loci to remove and the no_hi_misser genotypes
combined_df_loc <- combined_df %>% 
  anti_join(., outliers, by = c("locus" = "Locus"))

```

Check the number of samples
```{r}
combined_df_loc %>% 
  select(NMFS_DNA_ID) %>%
  unique()

```


## Self-assignment format

```{r spread-genos}
# first make integers of the alleles
alle_idxs <- combined_df_loc %>% 
  dplyr::select(NMFS_DNA_ID, locus, gene_copy, allele) %>%
  group_by(locus) %>%
  mutate(alleidx = as.integer(factor(allele, levels = unique(allele)))) %>%
  ungroup() %>%
  arrange(NMFS_DNA_ID, locus, alleidx) 
# select just the columns to retain and spread the alleles
alle_idx2 <- alle_idxs[,-4]
  
two_col <- alle_idx2 %>%
  unite(loc, locus, gene_copy, sep = ".") %>%
  spread(loc, alleidx)
```

### get it into the right format for rubias

```{r ids-grps}
# nofu ids for samples without duplicates, etc.
nofu_ids <- read_csv("data/nofu_ids_04022019.csv") %>%
  rename(collection = group)


combo_ids <- downsampled_bycatch_dataset %>%
  select(NMFS_DNA_ID, collection) %>%
  bind_rows(nofu_ids) %>%
  mutate(sample_type = "reference")


```


```{r add-meta}
two_col2 <- two_col %>%
  left_join(combo_ids) # add group identity

# reorder the columns
two_col3 <- two_col2[,c(268:269,1:267)]

# make the reference file
rubias_genos <- two_col3 %>%
  mutate(repunit = collection)

# reorder the columns
rubias_genos1 <- rubias_genos[,c(2,270,1,3:269)]

# rename indiv column
rubias_genos2 <- rubias_genos1 %>%
  rename(indiv = NMFS_DNA_ID) %>%
  filter(!is.na(collection))


```


```{r self-assign}
# Now that the data are in the correct format,
# perform self-assignment on baseline colony samples
sa_fulmars <- self_assign(reference = rubias_genos2, gen_start_col = 5)

# summarize repunit results
sa_to_repu <- sa_fulmars %>%
  group_by(indiv, collection, repunit) %>%
  top_n(1, scaled_likelihood) # just the top assignment for each sample
  
# summary of assignments 
assign_no_thres <- sa_to_repu %>%
  filter(scaled_likelihood > 0.9) %>%
  group_by(repunit, inferred_repunit) %>%
  tally()

```


1. double check collecion x date
Add collection date info
```{r}
rubias_genos2
```

```{r}
library(lubridate)
```

```{r}
# meta data
baseline_meta <- readRDS("data/meta-data-tibble.rds")

m1 <- baseline_meta %>%
  filter(str_detect(COLLECTION_DATE, "/")) 

m1$year <- year(mdy(m1$COLLECTION_DATE)) # make a new variable - Year

baseline_years <- m1 %>%
  select(NMFS_DNA_ID, year)
```



```{r}
# the date is in the "TAG_NUMBER" field
mix_meta <- meta %>%
  select(NMFS_DNA_ID, TAG_NUMBER) %>%
  filter(str_detect(TAG_NUMBER, "AK")) %>%
  separate(TAG_NUMBER, into = c("state", "year", "month", "day", "mon2", "day2"), sep = "-") %>%
  select(NMFS_DNA_ID, year)

m2 <- meta %>%
  select(NMFS_DNA_ID, TAG_NUMBER) %>%
  filter(!str_detect(TAG_NUMBER, "AK")) %>%
  separate(TAG_NUMBER, into = c("year", "month", "day", "mon2", "day2"), sep = "-") %>%
  select(NMFS_DNA_ID, year) %>%
  filter(!str_detect(year, "Alaska"))

# combine those dfs
years <- bind_rows(mix_meta, m2)

years$year <- as.numeric(years$year)

all_years <- years %>%
  bind_rows(baseline_years) %>%
  filter(!is.na(year)) %>%
  unique()

# this doesn't include the metadata for the baseline samples
rubias_temporal_genos <- rubias_genos2 %>%
  inner_join(all_years, by = c("indiv"="NMFS_DNA_ID")) %>% 
  unite(col = "collection", collection, year, sep = "_", remove = T) %>%
  #select(sample_type, repunit, collection, NMFS_DNA_ID, 5:270) %>%
  #rename(indiv = NMFS_DNA_ID) %>%
  mutate(repunit = collection)

rubias_temporal_genos %>%
  group_by(indiv) %>%
  add_tally() %>%
  filter(n > 1)
```

```{r rubias-temporal-analysis}
# perform self-assignment on baseline and bycatch samples
sa_fulmar_years <- self_assign(reference = rubias_temporal_genos, gen_start_col = 5)

# summarize repunit results
sa_to_repu_years <- sa_fulmar_years %>%
  group_by(indiv, collection, repunit) %>%
  top_n(1, scaled_likelihood) # just the top assignment for each sample
  
# summary of assignments
sa_to_repu_years %>%
  filter(scaled_likelihood > 0.9) %>%
  group_by(repunit, inferred_repunit) %>%
  tally()

```

```{r}
sa_to_repu_years %>%
  filter(scaled_likelihood > 0.9) %>%
  group_by(repunit, inferred_repunit) %>%
  ggplot(aes(x = collection, fill = inferred_collection)) +
  geom_bar(stat = "count") +
  theme_bw() +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.9)
  ) #+
  #scale_fill_manual(values = colors)
```

